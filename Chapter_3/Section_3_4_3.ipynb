{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782e6274",
   "metadata": {},
   "source": [
    "# Chapter 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7abc2e",
   "metadata": {},
   "source": [
    "# 3.4.3. Importing large datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a26a0d3",
   "metadata": {},
   "source": [
    "Here, we provide only example code without an accompanying data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4257d447",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the size of each chunk\n",
    "chunk_size = 10000  \n",
    "file_path = 'path/to/your/large_file.csv'\n",
    "\n",
    "# Using an iterator to load chunks of the file\n",
    "chunk_iter = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "\n",
    "# Print the first few rows of each chunk\n",
    "for chunk in chunk_iter:\n",
    "    print(chunk.head())\n",
    "\n",
    "# Example: Process each chunk and combine them into a final DataFrame\n",
    "processed_chunks = []\n",
    "for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "    # Perform data processing (e.g., filtering, aggregation, etc.)\n",
    "    processed_chunk = process_chunk(chunk)  # Here \"process_chunk\" is a defined function\n",
    "    processed_chunks.append(processed_chunk)\n",
    "\n",
    "# Concatenate all processed chunks into a single DataFrame\n",
    "final_df = pd.concat(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator function to read a large file line-by-line\n",
    "def read_large_file(file_obj):\n",
    "    \"\"\"A generator function to read a large file lazily.\"\"\"\n",
    "    while True:\n",
    "        data = file_obj.readline()\n",
    "        if not data:\n",
    "            break\n",
    "        yield data\n",
    "\n",
    "# Use the generator to read and process the file\n",
    "with open('path/to/your/large_file.txt', 'r') as file:\n",
    "    for line in read_large_file(file):\n",
    "        # Process each line (e.g., print or analyze)\n",
    "        print(line.strip())  # Replace with actual data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf056ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install h5py package\n",
    "%pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80427739",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Reading data from an HDF5 file\n",
    "with h5py.File('data.h5', 'r') as files:\n",
    "    data = files['my_dataset'][...]  # Load the dataset into memory\n",
    "    print(data)  # Display the content of the dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
