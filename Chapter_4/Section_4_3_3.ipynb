{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "782e6274",
   "metadata": {},
   "source": [
    "# Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f7abc2e",
   "metadata": {},
   "source": [
    "# 4.3.2. Techniques for handling missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce4bd0",
   "metadata": {},
   "source": [
    "A. Removal of incomplete records"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda933e2",
   "metadata": {},
   "source": [
    "Listwise deletion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b91d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Apply Listwise Deletion (removes any row with missing values)\n",
    "data_listwise_deleted = data.dropna()\n",
    "\n",
    "print(\"\\nDataFrame after Listwise Deletion:\")\n",
    "print(data_listwise_deleted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10273acc",
   "metadata": {},
   "source": [
    "Pairwise deletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9169cdcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Calculate pairwise correlations using pairwise deletion (excluding NA/null values)\n",
    "# The `pairwise deletion` is applied by using the 'min_periods=1' parameter \n",
    "# which will calculate correlation for available pairs without dropping rows entirely.\n",
    "correlation_matrix = data.corr(min_periods=1)\n",
    "\n",
    "print(\"\\nPairwise Correlation Matrix with Pairwise Deletion:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064648e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Threshold-based removal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab72037c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "print(data.shape)\n",
    "\n",
    "# Define a threshold for column removal (e.g., remove columns with more than 20% missing data)\n",
    "column_threshold = 0.2\n",
    "# Remove columns where the proportion of missing values is greater than the threshold\n",
    "df_column_threshold = data.loc[:, data.isnull().mean() <= column_threshold]\n",
    "print(\"\\nDataFrame after Column Threshold-based Removal (20% missing data threshold):\")\n",
    "print(df_column_threshold)\n",
    "print(df_column_threshold.shape)\n",
    "\n",
    "# Define a threshold for row removal (e.g., remove rows with more than 20% missing data)\n",
    "row_threshold = 0.2\n",
    "# Remove rows where the proportion of missing values is greater than the threshold\n",
    "df_row_threshold = data[data.isnull().mean(axis=1) <= row_threshold]\n",
    "print(\"\\nDataFrame after Row Threshold-based Removal (20% missing data threshold):\")\n",
    "print(df_row_threshold)\n",
    "print(df_row_threshold.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ce5b3a",
   "metadata": {},
   "source": [
    "B. Single Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7970fd",
   "metadata": {},
   "source": [
    "Mean/Median/Mode imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27055f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data2.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Separate numerical and categorical columns\n",
    "numerical_columns = data.select_dtypes(include=['number']).columns\n",
    "print(numerical_columns)\n",
    "\n",
    "# Mean Imputation: Replace missing values with the mean of the respective column (for numerical columns only)\n",
    "df_mean_imputed = data.copy()\n",
    "# Initialize SimpleImputer with mean strategy\n",
    "mean_imputer = SimpleImputer(strategy='mean')  \n",
    "df_mean_imputed[numerical_columns] = mean_imputer.fit_transform(df_mean_imputed[numerical_columns])\n",
    "print(\"\\nDataFrame after Mean Imputation (Numerical Columns):\")\n",
    "print(df_mean_imputed)\n",
    "\n",
    "# Median Imputation: Replace missing values with the median of the respective column (for numerical columns only)\n",
    "df_median_imputed = data.copy()\n",
    "# Initialize SimpleImputer with median strategy\n",
    "median_imputer = SimpleImputer(strategy='median')  \n",
    "df_median_imputed[numerical_columns] = median_imputer.fit_transform(df_median_imputed[numerical_columns])\n",
    "print(\"\\nDataFrame after Median Imputation (Numerical Columns):\")\n",
    "print(df_median_imputed)\n",
    "\n",
    "# Mode Imputation: Replace missing values with the mode of the respective column (useful for categorical columns)\n",
    "df_mode_imputed = data.copy()\n",
    "# Initialize SimpleImputer with mode strategy\n",
    "mode_imputer = SimpleImputer(strategy='most_frequent')  \n",
    "df_mode_imputed[data.columns] = mode_imputer.fit_transform(df_mode_imputed)\n",
    "print(\"\\nDataFrame after Mode Imputation (Numerical and Categorical Columns):\")\n",
    "print(df_mode_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a8709",
   "metadata": {},
   "source": [
    "Last observation carried forward (LOCF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbceea66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary library\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Apply LOCF (Last Observation Carried Forward) imputation\n",
    "df_locf = data.copy()  # Create a copy of the original dataframe\n",
    "df_locf = df_locf.ffill()  # Forward fill method to propagate last valid observation forward\n",
    "\n",
    "print(\"\\nDataFrame after LOCF Imputation:\")\n",
    "print(df_locf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375c9fd9",
   "metadata": {},
   "source": [
    "Hot deck imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f1cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data2.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Hot deck imputation function using grouping and random sampling\n",
    "def hot_deck_imputation(df, group_column):\n",
    "    \"\"\"\n",
    "    Perform hot deck imputation by filling missing values within groups using random sampling.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Dataframe with missing values\n",
    "    group_column (str): The column used to group similar records (e.g., 'Gender')\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Dataframe with missing values imputed\n",
    "    \"\"\"\n",
    "    # Create a copy of the dataframe to avoid modifying the original\n",
    "    df_imputed = df.copy()\n",
    "    \n",
    "    # Loop through each column in the dataframe that has missing values\n",
    "    for col in df.columns:\n",
    "        # Skip the group column itself\n",
    "        if col == group_column:\n",
    "            continue\n",
    "        \n",
    "        # Apply imputation within each group defined by the group_column\n",
    "        for group in df[group_column].unique():\n",
    "            group_mask = (df[group_column] == group)\n",
    "            missing_mask = df[col].isnull() & group_mask\n",
    "            \n",
    "            # Get available values within the group (non-missing values)\n",
    "            available_values = df.loc[group_mask & df[col].notnull(), col]\n",
    "            \n",
    "            # If there are available values to sample from, fill missing values with random sampling\n",
    "            if not available_values.empty:\n",
    "                df_imputed.loc[missing_mask, col] = np.random.choice(available_values, size=missing_mask.sum(), replace=True)\n",
    "\n",
    "    return df_imputed\n",
    "\n",
    "# Perform hot deck imputation using 'groups' as the grouping column\n",
    "df_hot_deck_imputed = hot_deck_imputation(data, group_column='groups')\n",
    "\n",
    "print(\"\\nDataFrame after Hot Deck Imputation (Grouped by 'groups'):\")\n",
    "print(df_hot_deck_imputed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7db6079",
   "metadata": {},
   "source": [
    "C. Model-based imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac46baf5",
   "metadata": {},
   "source": [
    "Regression imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3d2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Function for regression imputation\n",
    "def regression_imputation(df, target_column):\n",
    "    \"\"\"\n",
    "    Perform regression imputation on a column with missing values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with missing values.\n",
    "    target_column (str): The name of the column to perform regression imputation on.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: Column with missing values filled using regression imputation.\n",
    "    \"\"\"\n",
    "    # Separate rows with missing values and rows with observed values in the target column\n",
    "    observed_data = df[df[target_column].notnull()]\n",
    "    missing_data = df[df[target_column].isnull()]\n",
    "    \n",
    "    # Define the features (all columns except the target column)\n",
    "    X_train = observed_data.drop(columns=[target_column])\n",
    "    y_train = observed_data[target_column]\n",
    "\n",
    "    # Create a Linear Regression model\n",
    "    regressor = LinearRegression()\n",
    "\n",
    "    # Train the model using observed data\n",
    "    regressor.fit(X_train.fillna(0), y_train)\n",
    "\n",
    "    # Use the trained model to predict missing values\n",
    "    X_missing = missing_data.drop(columns=[target_column])\n",
    "    predicted_values = regressor.predict(X_missing.fillna(0))\n",
    "\n",
    "    # Fill missing values with the predicted values\n",
    "    df.loc[df[target_column].isnull(), target_column] = predicted_values\n",
    "\n",
    "    return df[target_column]\n",
    "\n",
    "# Apply regression imputation\n",
    "df = data.copy()\n",
    "df['feature1'] = regression_imputation(df, 'feature1')\n",
    "df['feature3'] = regression_imputation(df, 'feature3')\n",
    "df['feature4'] = regression_imputation(df, 'feature4')\n",
    "df['feature5'] = regression_imputation(df, 'feature5')\n",
    "\n",
    "print(\"\\nDataFrame after Regression Imputation:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12fbb32c",
   "metadata": {},
   "source": [
    "Stochastic regression imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14255c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Function for stochastic regression imputation\n",
    "def stochastic_regression_imputation(df, target_column):\n",
    "    \"\"\"\n",
    "    Perform stochastic regression imputation on a column with missing values.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input dataframe with missing values.\n",
    "    target_column (str): The name of the column to perform stochastic regression imputation on.\n",
    "\n",
    "    Returns:\n",
    "    pd.Series: Column with missing values filled using stochastic regression imputation.\n",
    "    \"\"\"\n",
    "    # Separate rows with missing values and rows with observed values in the target column\n",
    "    observed_data = df[df[target_column].notnull()]\n",
    "    missing_data = df[df[target_column].isnull()]\n",
    "    \n",
    "    # Define the features (all columns except the target column)\n",
    "    X_train = observed_data.drop(columns=[target_column])\n",
    "    y_train = observed_data[target_column]\n",
    "\n",
    "    # Create a Linear Regression model\n",
    "    regressor = LinearRegression()\n",
    "\n",
    "    # Train the model using observed data\n",
    "    regressor.fit(X_train.fillna(0), y_train)\n",
    "\n",
    "    # Use the trained model to predict missing values\n",
    "    X_missing = missing_data.drop(columns=[target_column])\n",
    "    predicted_values = regressor.predict(X_missing.fillna(0))\n",
    "\n",
    "    # Calculate the residuals (difference between observed and predicted values)\n",
    "    residuals = y_train - regressor.predict(X_train.fillna(0))\n",
    "\n",
    "    # Calculate the standard deviation of the residuals\n",
    "    residuals_std = np.std(residuals)\n",
    "\n",
    "    # Add random noise to the predicted values based on the residuals' standard deviation\n",
    "    stochastic_predictions = predicted_values + np.random.normal(0, residuals_std, size=predicted_values.shape)\n",
    "\n",
    "    # Fill missing values with the stochastic predictions\n",
    "    df.loc[df[target_column].isnull(), target_column] = stochastic_predictions\n",
    "\n",
    "    return df[target_column]\n",
    "\n",
    "# Apply stochastic regression imputation\n",
    "df = data.copy()\n",
    "df['feature1'] = stochastic_regression_imputation(df, 'feature1')\n",
    "df['feature3'] = stochastic_regression_imputation(df, 'feature3')\n",
    "df['feature4'] = stochastic_regression_imputation(df, 'feature4')\n",
    "df['feature5'] = stochastic_regression_imputation(df, 'feature5')\n",
    "\n",
    "print(\"\\nDataFrame after Stochastic Regression Imputation:\")\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "495d2d85",
   "metadata": {},
   "source": [
    "Multiple imputation using Scikit-learn's IterativeImputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e7fc7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Initialize imputer\n",
    "imputer = IterativeImputer()\n",
    "imputed = imputer.fit_transform(data)\n",
    "print(\"\\nDataFrame after Iterative Imputation:\")\n",
    "df = pd.DataFrame(imputed, index=data.index, columns=data.columns)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab35655",
   "metadata": {},
   "source": [
    "Multiple Imputation by Chained Equations (MICE) implementation from Statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cdab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from statsmodels.imputation import mice\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Initialize imputer\n",
    "mice_data = mice.MICEData(data)\n",
    "# Prepare MICE model formulas dynamically\n",
    "cols_with_missing = data.columns[data.isnull().any()].tolist()\n",
    "\n",
    "# Create a formula and perform MICE only for columns with missing data\n",
    "for column in cols_with_missing:\n",
    "    other_columns = list(data.columns.drop(column))  # All columns except the current one\n",
    "    formula = f\"{column} ~ \" + \" + \".join(other_columns)\n",
    "    mi_model = mice.MICE(formula, sm.OLS, mice_data)\n",
    "    mi_results = mi_model.fit(10, 10)  # Using 10 imputations with 10 iterations each\n",
    "    print(mi_results.summary())\n",
    "\n",
    "imputed_data = mice_data.data\n",
    "print(\"\\nDataFrame after MICE Imputation:\")\n",
    "print(imputed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77419b5f",
   "metadata": {},
   "source": [
    "K-Nearest Neighbors (KNN) imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cd349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Initialize imputer\n",
    "imputer = KNNImputer()\n",
    "imputed = imputer.fit_transform(data)\n",
    "df = pd.DataFrame(imputed, index=data.index, columns=data.columns)\n",
    "print(\"\\nDataFrame after KNN Imputation:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e96da6",
   "metadata": {},
   "source": [
    "Random forest imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea8e91bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('example_data/data.csv', index_col=0)\n",
    "print(\"Original DataFrame:\")\n",
    "print(data)\n",
    "\n",
    "# Initialize imputer\n",
    "imputer = IterativeImputer(estimator=RandomForestRegressor())\n",
    "imputed = imputer.fit_transform(data)\n",
    "\n",
    "df = pd.DataFrame(imputed, index=data.index, columns=data.columns)\n",
    "print(\"\\nDataFrame after Random Forest Imputation:\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3cab65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
